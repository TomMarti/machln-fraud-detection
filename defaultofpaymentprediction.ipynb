{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Default Prediction\n",
    "\n",
    "## Introduction\n",
    "This notebook explores credit card default prediction using machine learning. We'll analyze a dataset containing credit card client data to predict whether a client will default on their payment.\n",
    "\n",
    "### Dataset Overview\n",
    "- 30,000 credit card clients\n",
    "- Features include credit limit, gender, education, marital status, age, and payment history\n",
    "- Target variable: default payment next month (1 = default, 0 = no default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "First we'll load our data and take a look at its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/defaultofcreditcardclients.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows:\", df.shape[0])\n",
    "print(\"Number of columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Datas type\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['default payment next month'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(df.corr(), fignum=f.number)\n",
    "plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Pipeline Development\n",
    "\n",
    "We'll test three different models:\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Random Forest\n",
    "3. Logistic Regression\n",
    "\n",
    "Each model will be tested using a standardized pipeline including:\n",
    "- Feature scaling\n",
    "- Cross-validation\n",
    "- Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['default payment next month']\n",
    "X = df.drop(['ID', 'default payment next month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    # SVM parameters\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__kernel': ['linear', 'poly', 'rbf'],\n",
    "    \n",
    "    # Random Forest parameters\n",
    "    'rf__n_estimators': [100, 200, 300],\n",
    "    'rf__max_depth': [10, 20, None],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    \n",
    "    # Logistic Regression parameters\n",
    "    'lr__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__solver': ['liblinear', 'saga'],\n",
    "    'lr__max_iter': [1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Support Vector Machine': svm_pipeline,\n",
    "    'Random Forest': rf_pipeline,\n",
    "    'Logistic Regression': lr_pipeline\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "all_results = {}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nPerforming grid search for {name}...\")\n",
    "    \n",
    "    if name == 'Support Vector Machine':\n",
    "        current_params = {key: value for key, value in param_grid.items() if key.startswith('svm')}\n",
    "    elif name == 'Random Forest':\n",
    "        current_params = {key: value for key, value in param_grid.items() if key.startswith('rf')}\n",
    "    else:  # Logistic Regression\n",
    "        current_params = {key: value for key, value in param_grid.items() if key.startswith('lr')}\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=current_params,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        scoring=('accuracy', 'f1', 'f1_micro', 'f1_macro'),\n",
    "        refit='f1'\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    \n",
    "\n",
    "    all_scores = {\n",
    "        'accuracy': grid_search.cv_results_['mean_test_accuracy'][grid_search.best_index_],\n",
    "        'f1': grid_search.cv_results_['mean_test_f1'][grid_search.best_index_],\n",
    "        'f1_micro': grid_search.cv_results_['mean_test_f1_micro'][grid_search.best_index_],\n",
    "        'f1_macro': grid_search.cv_results_['mean_test_f1_macro'][grid_search.best_index_]\n",
    "    }\n",
    "\n",
    "    all_results[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'all_scores': all_scores,\n",
    "        'test_predictions': y_pred,\n",
    "        'classification_report': classification_report(y_test, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all_results is your dictionary containing all model results\n",
    "metrics = ['accuracy', 'f1', 'f1_micro', 'f1_macro']\n",
    "\n",
    "# Extract scores for each model\n",
    "scores = {}\n",
    "for model_name, model_data in all_results.items():\n",
    "    scores[model_name] = [\n",
    "        model_data['all_scores']['accuracy'],\n",
    "        model_data['all_scores']['f1'],\n",
    "        model_data['all_scores']['f1_micro'],\n",
    "        model_data['all_scores']['f1_macro']\n",
    "    ]\n",
    "\n",
    "# Create figure and subplots\n",
    "num_models = len(all_results)\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "gs = fig.add_gridspec(2, num_models + 1)\n",
    "\n",
    "# Metrics comparison plot\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# Bar plot setup\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.8 / num_models  # Adjust bar width based on number of models\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, num_models))\n",
    "\n",
    "# Plot bars for each model\n",
    "for i, (model_name, model_scores) in enumerate(scores.items()):\n",
    "    positions = x + width * (i - num_models/2 + 0.5)\n",
    "    bars = ax1.bar(positions, model_scores, width, label=model_name, color=colors[i])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    rotation=90)\n",
    "\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Model Performance Metrics Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics, rotation=45)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix plots\n",
    "for i, (model_name, model_data) in enumerate(all_results.items()):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    confusion_matrix = model_data['confusion_matrix']\n",
    "    \n",
    "    im = ax.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.set_title(f'Confusion Matrix\\n{model_name}')\n",
    "    \n",
    "    # Add labels to each cell\n",
    "    thresh = confusion_matrix.max() / 2.\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            ax.text(j, i, format(confusion_matrix[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
    "    \n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "\n",
    "# Add colorbar\n",
    "cbar_ax = fig.add_subplot(gs[1, -1])\n",
    "plt.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models demonstrate good accuracy, though F1 scores are lower across all models, with Random Forest performing the best. The micro F1 scores mirror the accuracy, and the macro F1 scores range between 0.63 and 0.69.\n",
    "\n",
    "The three models show good true negative detection, with acceptable true positive rates, though they all exhibit high false positive rates. This can be attributed to an imbalanced dataset (identified during exploration with a label mean of 0.22, indicating a majority of 0s). All models struggle with true predictions. It may be necessary to increase the representation of true cases in the dataset.\n",
    "\n",
    "The Random Forest model achieves the best balance among the three tested models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretability \n",
    "Let's analyze which features have the strongest influence on our predictions using SHAP (SHapley Additive exPlanations) values. On the best models: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, using no SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = 'Random Forest'\n",
    "\n",
    "best_model = best_models[best_model_name].named_steps['rf']\n",
    "\n",
    "importances = pd.Series(best_model.feature_importances_, index=X.columns)\n",
    "importances_sorted = importances.sort_values(ascending=True)  # Ascending=True for horizontal bars to show highest at top\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importances_sorted.index, importances_sorted.values)\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_values(shap_values, feature_names):\n",
    "    values = shap_values[0][:, 0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sorted_idx = np.argsort(np.abs(values))\n",
    "    \n",
    "    y_pos = np.arange(len(values))\n",
    "    plt.barh(y_pos, values[sorted_idx], \n",
    "            color=['red' if x < 0 else 'blue' for x in values[sorted_idx]])\n",
    "    \n",
    "    plt.yticks(y_pos, feature_names[sorted_idx], ha='right')\n",
    "    plt.xlabel('SHAP Value (Impact on Model Output)')\n",
    "    plt.title('Feature Importance (SHAP Values)')\n",
    "    \n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "plot_shap_values(shap_values, X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not balanced, need to be improved, SMOTE ? what manner. Grouping features correllated, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
